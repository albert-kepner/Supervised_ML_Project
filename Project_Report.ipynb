{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba928478",
   "metadata": {},
   "source": [
    "# Machine Learning to Detect Android Malware using Android App Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5aca4",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a472f3",
   "metadata": {},
   "source": [
    "This project uses a public data set of Android permissions collected from over 29000 benign and malware Android apps.\n",
    "The goal of my project is to explore several supervised ML algorithms and compare how effectively they \n",
    "can distinguish harmless apps from malware. The problem is of interest because computer malware \n",
    "on mobile devices has significant economic impact as well as violations of privacy. \n",
    "This is a supervised ML problem using a labeled data set. The task is binary classification -- determine whether a given app is \n",
    "likely to be malware or not\n",
    "based on the presence or absence of specific Android permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ca13c",
   "metadata": {},
   "source": [
    "### Project Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512dba6",
   "metadata": {},
   "source": [
    "https://github.com/albert-kepner/Supervised_ML_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52fae4",
   "metadata": {},
   "source": [
    "### The Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf68071",
   "metadata": {},
   "source": [
    "This project uses the NATICUSdroid (Android Permissions) Dataset from UCI ML data repository: https://archive.ics.uci.edu/ml/datasets.php.\n",
    "A link to this specific data set is here: https://archive-beta.ics.uci.edu/ml/datasets/naticusdroid+android+permissions+dataset .\n",
    "\n",
    "Citation: Mathur, Akshay & Mathur, Akshay. (2022). NATICUSdroid (Android Permissions) Dataset. UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9e532",
   "metadata": {},
   "source": [
    "The data set data.csv can be downloaded from the above website.\n",
    "The data set consists of 86 features which are either standard or customize Android permissions. These features were selected \n",
    "from a larger set possible Android permissions by the data set authors. These features have already been selected\n",
    "with the goal of maximizing discrimination between malware and benign apps. \n",
    "Each permission is either present or absent for a given app. \n",
    "So we have 86 columns containing 0 or 1 for the presence of a given permission.\n",
    "The last column of the data set is the label which is 0 for benign or 1 for malware. \n",
    "The data is already clean with no missing values.\n",
    "There are 29332 rows where each row represents 1 Android app known to be malware or not.\n",
    "14700 of the apps are malware, and 14632 are benign, so the two classes are evenly balanced.\n",
    "The data was collected from benign and malware Android applications over the period from 2010 to 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a30e03",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Feature Selection\n",
    "\n",
    "In this data set all the features are permissions encoded 0/1 as is the label. \n",
    "There are limited choices to display this data graphically. One thing of interest is\n",
    "how correlated the features are with each other. I created a correlation matrix and heat map showing all\n",
    "the pairwise correlations between features.\n",
    "\n",
    "See more details in the project notebook here: \n",
    "    \n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/Data_Set_And_Exploratory_Data_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53dc21",
   "metadata": {},
   "source": [
    "In the above notebook I also looked at the pairwise correlations between the 86 features. \n",
    "I eliminated one feature of each pair with the highest correlation until there were no pairs correlated above 0.90 .\n",
    "This process eliminated 12 features, leaving 74 feature columns.\n",
    "At the end of this notebook, I used sklearn.model_selection.train_test_split \n",
    "to save the training data (70%) and testing data (30%) off in separate CSV files train_data.csv and test_data.csv. This will\n",
    "make it convenient to train and evaluate multiple models on the same data in separate notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f1cff",
   "metadata": {},
   "source": [
    "## Training ML models on this dataset\n",
    "\n",
    "I set out to compare how well I could use this data set for prediction with several of the ML algorithms in this course. \n",
    "I also wanted to try out neural networks with keras/tensorflow on the same problem. \n",
    "Each of the models is in a separate notebook in my Github project. \n",
    "I created the following models which can viewed using these links:\n",
    "    \n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/Logistic_Regression_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/KNeighborsClassifier_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/Decision_Tree_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/RandomForest_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/AdaBoostClassifier_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/GradientBoostingClassifier_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/SupportVectorClassifier_Model.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/NeuralNetwork_Model1.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/NeuralNetwork_Model2.ipynb\n",
    "\n",
    "https://github.com/albert-kepner/Supervised_ML_Project/blob/master/NeuralNetwork_Model3.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07436f17",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "\n",
    "For each of the traditional ML models (all except neural networks) \n",
    "I used sklearn.model_selection.GridSearchCV to search for the best\n",
    "values of appropriate tuning parameters. For this tuning I used the\n",
    "default 5-fold cross validation with accuracy as the scoring parameter.\n",
    "At the end of each model notebook I used the best model found to predict\n",
    "based on the held out test data set, so that we have a final accuracy score\n",
    "which can be compared across models.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "For logistic regression I tried the following values for the 'C' parameter:\n",
    "    \n",
    "[0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, \n",
    "20.0, 50.0, 100.0, 200.0, 500.0, 1000.0, 2000.0, 5000.0]\n",
    "\n",
    "The best model based on cross validatation accuracy used C = 1000.\n",
    "\n",
    "## K Nearest Neighbors\n",
    "\n",
    "For this model the following parameters were tried:\n",
    "\n",
    "    parameters = {'n_neighbors': [1,3,5,7,9,11,15],\n",
    "             'weights': ['uniform','distance'], 'p':[1,2]}\n",
    "\n",
    "The best model used these parameters:\n",
    "    \n",
    "    {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}\n",
    "    \n",
    "## Decision Tree\n",
    "\n",
    "Parameters tried:\n",
    "    \n",
    "    parameters = {'max_depth':[3,5,7,10,12,13,15, 17], 'min_samples_leaf':[1,2,5,10]}\n",
    "    \n",
    "Best model:\n",
    "    \n",
    "    {'max_depth': 12, 'min_samples_leaf': 1}\n",
    "    \n",
    "## Random Forest\n",
    "\n",
    "Parameters tried:\n",
    "    \n",
    "    parameters = {'n_estimators': [100, 110, 120],\n",
    "              'max_depth': [20,22,24], \n",
    "              'min_samples_split': [2],\n",
    "             'min_samples_leaf': [1],\n",
    "             'max_features': ['log2'],\n",
    "             'ccp_alpha': [0.0, 0.005, 0.01]}\n",
    "Best model:\n",
    "    \n",
    "    {'ccp_alpha': 0.0,\n",
    " 'max_depth': 22,\n",
    " 'max_features': 'log2',\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 2,\n",
    " 'n_estimators': 120}\n",
    "    \n",
    "## AdaBoost Classifier\n",
    "\n",
    "Parameters tried:\n",
    "    \n",
    "    parameters = {'learning_rate': [0.85, 0.75, 0.65, 0.50], 'n_estimators': [50, 75, 90]}\n",
    "    \n",
    "Best model:\n",
    "    \n",
    "    {'learning_rate': 0.65, 'n_estimators': 90}\n",
    "    \n",
    "## Gradient Boosting Classifier\n",
    "\n",
    "Parameters tried:\n",
    "    \n",
    "    parameters = {'learning_rate': [0.05, 0.075, 0.10, 0.15], 'n_estimators': [100, 150, 200]}\n",
    "    \n",
    "Best model:\n",
    "    \n",
    "    {'learning_rate': 0.15, 'n_estimators': 200}\n",
    "    \n",
    "## Support Vector Classifier\n",
    "\n",
    "Parameters tried:\n",
    "    \n",
    "    parameters = {'C': [ 1.0, 2.0, 5.0, 10.0],\n",
    "              'kernel': ['linear','poly','rbf','sigmoid']}\n",
    "    \n",
    "Best model:\n",
    "    \n",
    "    {'C': 5.0, 'kernel': 'rbf'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48304009",
   "metadata": {},
   "source": [
    "## Neural Network Models\n",
    "\n",
    "Three models were tried. All three models have a single output node with \"sigmoid\" activation, \n",
    "which is appropriate for binary classification. The three models are:\n",
    "    \n",
    "* Model1 -- one dense hidden layer with 50 units\n",
    "* Model2 -- three dense hidden layers with 148/74/37 units in the layers\n",
    "* Model3 -- three dense hidden layers with 148/74/37 units in the layers plus 2 Dropout(rate=0.5) layers between the\n",
    "hidden layers\n",
    "\n",
    "The more complex models turned out to be slightly more accurate than Model1 but the difference was minor. \n",
    "All three models used a validation split of 0.2 (20% of data held out for validatation) during training. \n",
    "All three models early stopping based on validation loss to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8ef8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
